{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE is : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "# setup optimal acceleration DEVICE \n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")  # Use Metal Performance Shaders on macOS\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")  # to check if cuda is an option https://www.restack.io/p/gpu-computing-answer-is-my-gpu-cuda-enabled-cat-ai\n",
    "\n",
    "print(f\"DEVICE is : {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_tokenizer(model_path_or_id:str, device:str):\n",
    "    # Replace AutoModelForMaskedLM with the correct class for your task, e.g., AutoModelForSequenceClassification\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path_or_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreival function\n",
    "def top_k_prediction(masked_text, model, tokenizer, k=10):\n",
    "    model.to(DEVICE) \n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = model(**inputs).logits\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"]==tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    return [tokenizer.decode(t) for t in torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS\n",
    "def recall_in_top_k(A,B): \n",
    "    overlap = {word for word in A if word in B}\n",
    "    return overlap\n",
    "\n",
    "def differences_in_top_k(A,B): \n",
    "    new_in_B = {word for word in B if word not in A}\n",
    "    left_out_in_B= {word for word in A if word not in B}\n",
    "    return new_in_B, left_out_in_B\n",
    "\n",
    "#jaccard Similarity: Measures overlap between two sets or lists.\n",
    "def jaccard(A,B): \n",
    "    intersection = {word for word in A if word in B}\n",
    "    union = set(A+B)\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "\n",
    "#perplexity of target words\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_masked_word_perplexity(target_word, masked_text, model, tokenizer):\n",
    "    model.to(DEVICE) \n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    # Convert target word to ID\n",
    "    target_word_id = tokenizer.convert_tokens_to_ids(target_word)\n",
    "    \n",
    "    #mask_token_index = torch.where(inputs[\"input_ids\"]==tokenizer.mask_token_id)[1] #idk why this breaks ? \n",
    "    mask_token_index = (inputs[\"input_ids\"][0] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "    # Extract logits for the masked token position\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = F.softmax(mask_token_logits, dim=-1)\n",
    "    \n",
    "    p_word = probabilities[target_word_id].item()\n",
    "    \n",
    "    return 1 / p_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilbert/distilbert-base-uncased'\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(model_path_or_id=model_checkpoint, device=DEVICE)\n",
    "extended_model , tokenizer = load_model_tokenizer(\"movie_model\\checkpoint-958\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  {'deal', 'gift', '?', 'adventure', '!', 'fun', 'mess', 'night', '.', 'story', 'job', ';', 'time', 'day', 'thing', 'idea', 'surprise', 'song', 'chance'}\n",
      "differences:  ({'film', 'show', 'book', 'game', 'one', 'effort', 'way', 'plan', 'movie', 'performance', ','}, {'mystery', 'beauty', 'wonder', 'treasure', 'coincidence', 'prize', 'disaster', 'boy', 'success', 'fortune', 'tragedy'})\n",
      "Jacard Score:  0.4634146341463415\n",
      "Perplexity for the word movie, in What a great [MASK] base model: 3957.4228697477365\n",
      "Perplexity for the word movie, in What a great [MASK] extend model: 454.4608551738091 \n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "#recall (how many words stayed the same)\n",
    "text = \"What a great [MASK]\"\n",
    "A = list(top_k_prediction(text, model, tokenizer, k))\n",
    "B = list(top_k_prediction(text, extended_model, tokenizer, k))\n",
    "\n",
    "#metrics on wordlist inputs\n",
    "print(\"recall: \", recall_in_top_k(A,B))\n",
    "print(\"differences: \", differences_in_top_k(A,B))\n",
    "print(\"Jacard Score: \", jaccard(A,B))\n",
    "#metrics on model predictions (probs)\n",
    "target_word = \"movie\"\n",
    "print(f\"Perplexity for the word {target_word}, in {text} base model: {compute_masked_word_perplexity(target_word, text, model, tokenizer)}\")\n",
    "print(f\"Perplexity for the word {target_word}, in {text} extend model: {compute_masked_word_perplexity(target_word, text, extended_model, tokenizer)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
